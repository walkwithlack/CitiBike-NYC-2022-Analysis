{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79aece80-cf0b-4eb2-9191-5140c86ffc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29,838,166 trip records\n",
      "\n",
      "[1/2] Extracting station metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magia\\anaconda3\\envs\\CB22\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1955: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved stations_metadata.csv (300 stations)\n",
      "\n",
      "[2/2] Extracting hourly flows...\n",
      "✓ Saved hourly_flows.csv (50400 records)\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. stations_metadata.csv - 15.2 KB\n",
      "  2. hourly_flows.csv - 1.3 MB\n",
      "\n",
      "Total size: 1.33 MB (from 5 GB!)\n",
      "Reduction: 99.97%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract only the essential data needed for the Streamlit bike redistribution map.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import math\n",
    "\n",
    "# ==================================================================================\n",
    "# CONFIGURATION (same as your Streamlit app)\n",
    "# ==================================================================================\n",
    "TOP_N = 300\n",
    "TARGET_STATIONS_PER_ZONE = 12\n",
    "INPUT_FILE = r\"C:\\Users\\magia\\OneDrive\\Desktop\\NY_Citi_Bike\\2.Data\\Prepared Data\\nyc_2022_essential_data.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\magia\\OneDrive\\Desktop\\NY_Citi_Bike\\2.Data\\Prepared Data\\aggregated\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    INPUT_FILE,\n",
    "    low_memory=False,\n",
    "    on_bad_lines=\"skip\",\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(df):,} trip records\")\n",
    "\n",
    "# ==================================================================================\n",
    "# STEP 1: Extract Station Metadata (top 300 stations with coords and zones)\n",
    "# ==================================================================================\n",
    "print(\"\\n[1/2] Extracting station metadata...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "for col in [\"started_at\", \"ended_at\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Get station coordinates (median lat/lon per station)\n",
    "starts_geo = (df.dropna(subset=[\"start_station_name\", \"start_lat\", \"start_lng\"])\n",
    "              .groupby(\"start_station_name\")[[\"start_lat\", \"start_lng\"]]\n",
    "              .median()\n",
    "              .rename(columns={\"start_lat\": \"lat\", \"start_lng\": \"lon\"}))\n",
    "\n",
    "ends_geo = (df.dropna(subset=[\"end_station_name\", \"end_lat\", \"end_lng\"])\n",
    "            .groupby(\"end_station_name\")[[\"end_lat\", \"end_lng\"]]\n",
    "            .median()\n",
    "            .rename(columns={\"end_lat\": \"lat\", \"end_lng\": \"lon\"}))\n",
    "\n",
    "stations_all = starts_geo.combine_first(ends_geo)\n",
    "stations_all.index.name = \"station\"\n",
    "\n",
    "# Get station volumes\n",
    "starts_count = df.groupby(\"start_station_name\").size().rename(\"starts\")\n",
    "ends_count = df.groupby(\"end_station_name\").size().rename(\"ends\")\n",
    "station_volume = (pd.concat([starts_count, ends_count], axis=1)\n",
    "                  .fillna(0)\n",
    "                  .assign(total=lambda x: x.starts + x.ends)\n",
    "                  .sort_values(\"total\", ascending=False))\n",
    "\n",
    "# Keep top N stations\n",
    "top_stations = (station_volume.head(TOP_N)\n",
    "                .join(stations_all, how=\"left\")\n",
    "                .dropna(subset=[\"lat\", \"lon\"])\n",
    "                .copy())\n",
    "\n",
    "# Cluster into geographic zones\n",
    "n_clusters = max(1, math.ceil(len(top_stations) / TARGET_STATIONS_PER_ZONE))\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=256, n_init=\"auto\")\n",
    "top_stations[\"geo_zone\"] = kmeans.fit_predict(top_stations[[\"lat\", \"lon\"]])\n",
    "\n",
    "# Renumber zones 0, 1, 2, ... for consistency\n",
    "zone_mapping = {old: new for new, old in enumerate(sorted(top_stations[\"geo_zone\"].unique()))}\n",
    "top_stations[\"geo_zone\"] = top_stations[\"geo_zone\"].map(zone_mapping).astype(int)\n",
    "\n",
    "# Save station metadata\n",
    "# The index contains station names, so we need to reset it and rename properly\n",
    "stations_metadata = top_stations.reset_index()\n",
    "stations_metadata = stations_metadata[[stations_metadata.columns[0], \"lat\", \"lon\", \"geo_zone\", \"total\"]]\n",
    "stations_metadata.columns = [\"station_name\", \"lat\", \"lon\", \"geo_zone\", \"total_trips\"]\n",
    "stations_metadata.to_csv(f\"{OUTPUT_DIR}/stations_metadata.csv\", index=False)\n",
    "print(f\"✓ Saved stations_metadata.csv ({len(stations_metadata)} stations)\")\n",
    "\n",
    "# ==================================================================================\n",
    "# STEP 2: Extract Hourly Flows per Zone\n",
    "# ==================================================================================\n",
    "print(\"\\n[2/2] Extracting hourly flows...\")\n",
    "\n",
    "station_to_zone = top_stations[\"geo_zone\"].to_dict()\n",
    "\n",
    "# Calculate hourly starts per zone\n",
    "starts_hourly = (df.loc[df[\"start_station_name\"].isin(top_stations.index)]\n",
    "                 .dropna(subset=[\"started_at\"])\n",
    "                 .assign(zone=lambda x: x[\"start_station_name\"].map(station_to_zone),\n",
    "                         hour=lambda x: x[\"started_at\"].dt.hour,\n",
    "                         dow=lambda x: x[\"started_at\"].dt.dayofweek,\n",
    "                         month=lambda x: x[\"started_at\"].dt.to_period(\"M\").astype(str))\n",
    "                 .groupby([\"month\", \"dow\", \"hour\", \"zone\"]).size()\n",
    "                 .rename(\"starts\").reset_index())\n",
    "\n",
    "# Calculate hourly ends per zone\n",
    "ends_hourly = (df.loc[df[\"end_station_name\"].isin(top_stations.index)]\n",
    "               .dropna(subset=[\"ended_at\"])\n",
    "               .assign(zone=lambda x: x[\"end_station_name\"].map(station_to_zone),\n",
    "                       hour=lambda x: x[\"ended_at\"].dt.hour,\n",
    "                       dow=lambda x: x[\"ended_at\"].dt.dayofweek,\n",
    "                       month=lambda x: x[\"ended_at\"].dt.to_period(\"M\").astype(str))\n",
    "               .groupby([\"month\", \"dow\", \"hour\", \"zone\"]).size()\n",
    "               .rename(\"ends\").reset_index())\n",
    "\n",
    "# Create complete grid (all months × days × hours × zones, even if zero trips)\n",
    "all_months = sorted(set(starts_hourly[\"month\"]) | set(ends_hourly[\"month\"]))\n",
    "all_dows = sorted(set(starts_hourly[\"dow\"]) | set(ends_hourly[\"dow\"]))\n",
    "all_hours = range(24)\n",
    "all_zones = sorted(top_stations[\"geo_zone\"].unique())\n",
    "\n",
    "grid = pd.MultiIndex.from_product(\n",
    "    [all_months, all_dows, all_hours, all_zones],\n",
    "    names=[\"month\", \"dow\", \"hour\", \"zone\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Merge with actual data and fill missing with zeros\n",
    "hourly_flows = (grid.merge(starts_hourly, on=[\"month\", \"dow\", \"hour\", \"zone\"], how=\"left\")\n",
    "                .merge(ends_hourly, on=[\"month\", \"dow\", \"hour\", \"zone\"], how=\"left\")\n",
    "                .fillna({\"starts\": 0, \"ends\": 0}))\n",
    "\n",
    "# Save hourly flows\n",
    "hourly_flows.to_csv(f\"{OUTPUT_DIR}/hourly_flows.csv\", index=False)\n",
    "print(f\"✓ Saved hourly_flows.csv ({len(hourly_flows)} records)\")\n",
    "\n",
    "# ==================================================================================\n",
    "# SUMMARY\n",
    "# ==================================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "stations_size = os.path.getsize(f\"{OUTPUT_DIR}/stations_metadata.csv\") / 1024\n",
    "flows_size = os.path.getsize(f\"{OUTPUT_DIR}/hourly_flows.csv\") / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  1. stations_metadata.csv - {stations_size:.1f} KB\")\n",
    "print(f\"  2. hourly_flows.csv - {flows_size:.1f} MB\")\n",
    "print(f\"\\nTotal size: {stations_size/1024 + flows_size:.2f} MB (from 5 GB!)\")\n",
    "print(f\"Reduction: {100 * (1 - (stations_size/1024 + flows_size) / 5000):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb53b4c-207d-4d08-a61a-48e59e5c2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_metadata.to_csv('stations_metadata.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f648ed-8d2c-4f1d-a2a7-0791fd94ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_flows.to_csv('hourly_flows.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

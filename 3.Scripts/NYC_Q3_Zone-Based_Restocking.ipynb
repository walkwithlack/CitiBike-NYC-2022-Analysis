{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf3d957-22a1-4158-b496-d958adff9c95",
   "metadata": {},
   "source": [
    "# Question: What are some ideas for ensuring bikes are always stocked at the most popular stations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663db625-13df-4372-967c-8f5d6aaa81e9",
   "metadata": {},
   "source": [
    "### Problem Framing\n",
    "\n",
    "Two rather easy ideas are integrating an event calendar to anticipate changes in bike usage & introducing rider incentives for bike redistribution in selected stations (after all, the vast majority of bikes are moved by riders/customers and not ops!). To be able to do this, as well as to be able to run operations at all, we need to know where and when to intevene and when to refrain from doing anything. We need to know whether a station is at a particular time a big source (net trip origins) or is going to become one soon enough, while simultaneously identifying sinks (net trip destinations), since bikes moved must come from other stations.\n",
    "\n",
    "Bike availability depends on usage, which fluctuates at ***multiple time scales***: months (temperature effects), days of week (weekday vs weekend patterns), and hours (commuter peaks). ***These scales affect the system differently***—monthly variation impacts total volume, while hourly patterns shape station behavior. Stations shift from sinks in morning rush hours to sources during evening commutes, reflecting commuters' first-mile and last-mile needs.\n",
    "\n",
    "Some stations are both big sinks and sources simultaneously (***bidirectional demand***), even if one \"personality\" dominates. This means we're ***not seeking net flow of zero***—at certain hours, a balanced station may still need intervention to maintain capacity for expected demand patterns.\n",
    "\n",
    "As redistribution depends on vans ***road constraints***, we avoided clustering stations by trip flows (high-volume pairs aren't necessarily proximate) and administrative boundaries (bike use ignores these).\n",
    "\n",
    "### Solution Approach\n",
    "\n",
    "We built a zone-based rebalancing model through eight steps:\n",
    "1. Geographic zones: K-means clustering on lat/lon coordinates grouped the top 300 stations into 25 zones, ensuring operationally feasible redistribution routes. Zones remain static; their roles and imbalance magnitudes change over time.\n",
    "2. Zone-level block detection: For each zone×month×dow, we detect time blocks by analyzing 24-hour net flow patterns (C = ends − starts). The algorithm identifies continuous periods where smoothed C remains consistently positive (supply) or negative (demand), meeting minimum duration (2 hours) and magnitude (|C| ≥ 6 bikes) thresholds.\n",
    "3. Two-regime consolidation: Overnight (23:00-04:00) gets one canonical block per month×dow by summing hourly C values—any zone with |C| ≥ 6 participates. Daytime (04:00-23:00) blocks consolidate using 3-hour tolerance, merging similar boundaries into canonical windows. This reduced 4,644 fragmented blocks to 731 (~9 per month×dow).\n",
    "4. Day-level zone profiling: We identify self-correcting residential zones showing morning deficits (C < -20), evening surpluses (C > +20), and quiet middays (|C| < 15). These zones balance naturally—morning's empty docks accommodate evening returns. We found 45 such combinations across 15 zones and excluded 311 block assignments.\n",
    "5. Bidirectional demand adjustment: We calculate rental pressure (starts ÷ total activity) and return pressure (ends ÷ total activity) for each zone×block. Supply zones scale movable quantity by return pressure (preserving rental capacity); demand zones scale by rental pressure (acknowledging ongoing returns). This reduced movable quantities to 53% of raw |C|, preventing the model from stripping busy stations bare.\n",
    "6. Sticky zones heuristic: Zones flipping roles between consecutive blocks (supply → demand or vice versa) have quantities reduced by 50%. We identified 2,737 sticky blocks (31.7%), preventing wasteful back-and-forth moves.\n",
    "7. Greedy matching: We match supply to demand zones using nearest-neighbor distance (Haversine formula), moving min(supply available, demand needed) bikes per route until exhausted or max routes reached.\n",
    "8. User controls: A β parameter (0.1–0.9) scales all quantities (aggressive vs conservative rebalancing), with additional filters for minimum moves and maximum routes displayed.\n",
    "The interactive map lets users select month, day-of-week, and time block to view zones needing intervention, their supply/demand status, and suggested redistribution routes—grounded in 2022 patterns and operational constraints.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "1. Our model uses historical 2022 data only (no real-time updates; missing traffic, terrain, precipitation, staffing data), thereby treating monthly averages as deterministic forecasts without uncertainty quantification.\n",
    "2. Block consolidation creates temporal ambiguity: Consolidating zone-detected blocks (±3 hour tolerance) reduces fragmentation (4,644 → 731) but forces zones with different timing onto shared labels, obscuring precise intervention timing. Additionally, consolidated blocks sometimes overlap (e.g., 04:00-13:00, 05:00-09:00, 08:00-16:00 coexisting), creating dropdown redundancy. This is partially mitigated by focusing on top 300 Manhattan stations with synchronized demand patterns.\n",
    "3. Threshold sensitivity: Arbitrary thresholds (minimum |C| = 6, residential cutoffs, sticky penalty = 50%, β = 0.1-0.9) are tuning parameters lacking principled derivation. Small changes significantly alter recommendations without systematic optimization.\n",
    "4. Greedy matching suboptimality: In the absence of linear programming skills, we use nearest-neighbor heuristics—computationally simple but not guaranteed optimal for total route distance or operational efficiency.\n",
    "5. Smoothing introduces temporal blur: The 3-hour centered moving average stabilizes regime detection but blurs precise demand shift timing. Sharp 9:00am transitions may appear as gradual 8:30-9:30am shifts, misaligning blocks with operational reality.\n",
    "\n",
    "We proceeded with these limitations as way of engaging with the question's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6dd194-d523-48e9-900f-f02d761f6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "Will process top 300 stations into ~25 zones\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Loading data...\n",
      "================================================================================\n",
      "Loaded 29,838,166 trip records\n",
      "Date range: 2022-01-01 00:00:13.532000 to 2022-12-31 23:58:19.206000\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Identifying top stations and creating geographic zones...\n",
      "================================================================================\n",
      "Selected top 300 stations\n",
      "Total trips covered: 32,076,043 (107.5% of all trips)\n",
      "Created 25 geographic zones\n",
      "Average stations per zone: 12.0\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Calculating hourly flows per zone...\n",
      "================================================================================\n",
      "Created baseline with 50,400 records\n",
      "Coverage: 12 months × 7 days × 24 hours × 25 zones\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Detecting time blocks per zone...\n",
      "================================================================================\n",
      "Detected 7371 raw blocks\n",
      "Average blocks per zone×month×dow: 3.5\n",
      "\n",
      "Sample blocks:\n",
      "     month  dow  zone  hour_from  hour_to  C_expected  action\n",
      "0  2022-01    0     0          0        2        -6.0  demand\n",
      "1  2022-01    0     2          0        3       -12.0  demand\n",
      "2  2022-01    0     3          0        4       -13.0  demand\n",
      "3  2022-01    0     5          0        3        19.0  supply\n",
      "4  2022-01    0     7          0        3       -15.0  demand\n",
      "5  2022-01    0     9          0        2       -10.0  demand\n",
      "6  2022-01    0    10          0        2       -17.0  demand\n",
      "7  2022-01    0    11          0        4        24.0  supply\n",
      "8  2022-01    0    13          0        2         9.0  supply\n",
      "9  2022-01    0    15          0        2        -9.0  demand\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Consolidating similar blocks...\n",
      "================================================================================\n",
      "Unique blocks BEFORE consolidation: 4644\n",
      "\n",
      "Creating canonical overnight blocks (23:00-04:00)...\n",
      "  Generated 1845 overnight zone assignments\n",
      "\n",
      "Filtering daytime blocks (04:00-23:00)...\n",
      "  Kept 3450 daytime blocks from raw detection\n",
      "\n",
      "Consolidating daytime blocks (tolerance=3h)...\n",
      "  Consolidated to 7109 daytime zone assignments\n",
      "\n",
      "Unique blocks AFTER consolidation: 731\n",
      "Reduction: 4644 → 731 (84.3% reduction)\n",
      "\n",
      "Overnight blocks (23:00-04:00): 84 found\n",
      "✓ Canonical overnight block successfully enforced\n",
      "\n",
      "Blocks per month×dow after consolidation (sample):\n",
      "2022-01 dow=0: 8 blocks - ['04:00–13:00', '05:00–08:00', '06:00–11:00', '11:00–21:00', '12:00–15:00', '13:00–22:00', '14:00–19:00', '23:00–04:00']\n",
      "2022-01 dow=1: 7 blocks - ['05:00–12:00', '05:00–09:00', '09:00–12:00', '10:00–20:00', '13:00–16:00', '14:00–20:00', '23:00–04:00']\n",
      "2022-01 dow=2: 10 blocks - ['04:00–12:00', '05:00–08:00', '07:00–12:00', '09:00–18:00', '12:00–15:00', '14:00–20:00', '14:00–19:00', '15:00–21:00', '19:00–22:00', '23:00–04:00']\n",
      "2022-01 dow=3: 8 blocks - ['05:00–12:00', '05:00–11:00', '09:00–13:00', '11:00–19:00', '12:00–22:00', '13:00–18:00', '15:00–21:00', '23:00–04:00']\n",
      "2022-01 dow=4: 8 blocks - ['04:00–13:00', '05:00–09:00', '08:00–16:00', '10:00–12:00', '13:00–21:00', '13:00–17:00', '15:00–21:00', '23:00–04:00']\n",
      "\n",
      "================================================================================\n",
      "BASELINE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Key objects created:\n",
      "- stations_top: 300 stations\n",
      "- baseline_hourly: 50400 records\n",
      "- moves_blocks: 8954 blocks (after consolidation)\n",
      "- blocks_catalog: 731 unique time windows (after consolidation)\n",
      "\n",
      "Average blocks per month×dow: 8.7\n",
      "\n",
      "Ready for next steps:\n",
      "1. ✓ Block consolidation (COMPLETE)\n",
      "2. Day-level zone profiling\n",
      "3. Bidirectional demand adjustment\n",
      "4. Sticky zones heuristic\n",
      "5. Interactive map\n",
      "\n",
      "================================================================================\n",
      "STEP 6: Day-level zone profiling (identify self-correcting zones)...\n",
      "================================================================================\n",
      "Found 45 self-correcting zone×month×dow combinations\n",
      "Unique zones flagged: 15 out of 25\n",
      "\n",
      "Sample self-correcting zones:\n",
      "     month  dow  zone  morning_C  evening_C  midday_max\n",
      "0  2022-01    5    22     -268.0       81.0         0.0\n",
      "1  2022-01    6     5     -108.0       68.0         0.0\n",
      "2  2022-01    6     7      -53.0       59.0         9.0\n",
      "3  2022-02    3     1     -601.0      426.0         0.0\n",
      "4  2022-02    3    23     -337.0      201.0         0.0\n",
      "5  2022-02    4     1     -289.0      264.0         0.0\n",
      "6  2022-02    5    15      -84.0       75.0         0.0\n",
      "7  2022-02    5    17     -436.0       24.0         0.0\n",
      "8  2022-02    6     7      -22.0       48.0         0.0\n",
      "9  2022-03    3     4     -556.0      100.0         0.0\n",
      "\n",
      "Excluded 311 block assignments from self-correcting zones\n",
      "Remaining blocks: 8643\n",
      "Updated catalog: 731 unique time windows\n",
      "\n",
      "✓ Day-level zone profiling COMPLETE\n",
      "\n",
      "================================================================================\n",
      "STEP 7: Bidirectional demand adjustment...\n",
      "================================================================================\n",
      "Adding starts/ends data from baseline_hourly...\n",
      "Enriched 8643 block assignments\n",
      "\n",
      "Sample enriched data:\n",
      "     month  dow  zone  hour_from  hour_to  C_expected  starts    ends  \\\n",
      "0  2022-01    0     0          4       13       434.0  1590.0  2027.0   \n",
      "1  2022-01    0     1          4       13      -395.0  1698.0  1357.0   \n",
      "2  2022-01    0     2          4       13       519.0  1814.0  2302.0   \n",
      "3  2022-01    0     5          4       13      -147.0   444.0   297.0   \n",
      "4  2022-01    0     7          4       13       204.0  1026.0  1204.0   \n",
      "5  2022-01    0     8          4       13        79.0   986.0  1077.0   \n",
      "6  2022-01    0    10          4       13      -190.0  1587.0  1394.0   \n",
      "7  2022-01    0    11          4       13      -678.0  2221.0  1576.0   \n",
      "8  2022-01    0    12          4       13       677.0  1140.0  1804.0   \n",
      "9  2022-01    0    13          4       13      -109.0   612.0   529.0   \n",
      "\n",
      "   rental_pressure  return_pressure  \n",
      "0         0.439591         0.560409  \n",
      "1         0.555810         0.444190  \n",
      "2         0.440719         0.559281  \n",
      "3         0.599190         0.400810  \n",
      "4         0.460090         0.539910  \n",
      "5         0.477945         0.522055  \n",
      "6         0.532372         0.467628  \n",
      "7         0.584935         0.415065  \n",
      "8         0.387228         0.612772  \n",
      "9         0.536372         0.463628  \n",
      "\n",
      "Movable quantities calculated (before β scaling)\n",
      "Average adjustment factor: 0.53\n",
      "\n",
      "Examples of bidirectional adjustment:\n",
      "  Zone 0, supply: C=434, starts=1590, ends=2027 → movable=243 (56% of |C|)\n",
      "  Zone 1, demand: C=-395, starts=1698, ends=1357 → movable=220 (56% of |C|)\n",
      "  Zone 2, supply: C=519, starts=1814, ends=2302 → movable=290 (56% of |C|)\n",
      "  Zone 5, demand: C=-147, starts=444, ends=297 → movable=88 (60% of |C|)\n",
      "  Zone 7, supply: C=204, starts=1026, ends=1204 → movable=110 (54% of |C|)\n",
      "\n",
      "✓ Bidirectional demand adjustment COMPLETE\n",
      "\n",
      "================================================================================\n",
      "STEP 8: Sticky zones heuristic (reduce moves for role-flipping zones)...\n",
      "================================================================================\n",
      "Checking for zones that flip roles between consecutive blocks...\n",
      "Found 2737 sticky blocks out of 8643 (31.7%)\n",
      "These will have movable quantities reduced by 50%\n",
      "\n",
      "Sample sticky zones (role-flippers):\n",
      "  Zone 1, 2022-01 dow=0, 04:00-13:00 (demand): movable 220 → 110\n",
      "  Zone 1, 2022-01 dow=0, 14:00-19:00 (supply): movable 148 → 74\n",
      "  Zone 2, 2022-01 dow=0, 06:00-11:00 (supply): movable 289 → 145\n",
      "  Zone 3, 2022-01 dow=0, 12:00-15:00 (supply): movable 57 → 28\n",
      "  Zone 4, 2022-01 dow=0, 06:00-11:00 (demand): movable 55 → 28\n",
      "  Zone 5, 2022-01 dow=0, 06:00-11:00 (demand): movable 93 → 46\n",
      "  Zone 6, 2022-01 dow=0, 06:00-11:00 (supply): movable 9 → 5\n",
      "  Zone 6, 2022-01 dow=0, 12:00-15:00 (demand): movable 7 → 3\n",
      "  Zone 6, 2022-01 dow=0, 12:00-15:00 (supply): movable 3 → 2\n",
      "  Zone 6, 2022-01 dow=0, 13:00-22:00 (demand): movable 3 → 1\n",
      "\n",
      "✓ Sticky zones heuristic COMPLETE\n",
      "\n",
      "================================================================================\n",
      "MODEL BUILD COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final statistics:\n",
      "- Stations: 300\n",
      "- Zones: 25\n",
      "- Time blocks: 731 unique windows\n",
      "- Block assignments: 8643 (after all filtering)\n",
      "- Average blocks per month×dow: 8.7\n",
      "\n",
      "Features implemented:\n",
      "✓ Geographic zone clustering\n",
      "✓ Zone-level block detection\n",
      "✓ Block consolidation (tolerance=3h)\n",
      "✓ Canonical overnight block (23:00-04:00)\n",
      "✓ Day-level zone profiling (self-correcting zones)\n",
      "✓ Bidirectional demand adjustment\n",
      "✓ Sticky zones heuristic (role-flippers)\n",
      "\n",
      "Ready for:\n",
      "- Interactive map visualization\n",
      "- User-controlled β parameter for intervention scaling\n",
      "- Greedy matching algorithm for supply→demand routes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CITIBIKE REBALANCING MODEL - COMPLETE IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\magia\\OneDrive\\Desktop\\NY_Citi_Bike\\2.Data\\Prepared Data\\nyc_2022_essential_data.csv\"\n",
    "\n",
    "# Station filtering\n",
    "TOP_N = 300  # Keep top N stations by volume\n",
    "\n",
    "# Zone clustering\n",
    "TARGET_STATIONS_PER_ZONE = 12  # Approximate stations per geographic zone\n",
    "\n",
    "# Block detection parameters\n",
    "SMOOTH_WINDOW = 3      # Hours for moving average smoothing\n",
    "EPS = 1.0              # Deadband threshold (treat |C| < EPS as neutral)\n",
    "MIN_BLOCK_HOURS = 2    # Minimum block duration\n",
    "MIN_ABS_C = 6          # Minimum |total C| to create a block\n",
    "\n",
    "# Block consolidation\n",
    "CONSOLIDATION_TOLERANCE = 3  # Hours tolerance for merging similar blocks\n",
    "FORCE_OVERNIGHT_BLOCK = True  # Force a canonical overnight block\n",
    "OVERNIGHT_START = 23  # Overnight block starts at 23:00 (11 PM)\n",
    "OVERNIGHT_END = 4     # Overnight block ends at 04:00 (4 AM)\n",
    "\n",
    "# Zone profiling (self-correcting zones)\n",
    "RESIDENTIAL_MORNING_THRESHOLD = -20   # Morning deficit threshold\n",
    "RESIDENTIAL_EVENING_THRESHOLD = 20    # Evening surplus threshold\n",
    "RESIDENTIAL_MIDDAY_THRESHOLD = 15     # Max midday activity\n",
    "\n",
    "# Sticky zones (role-flipping penalty)\n",
    "FLIP_PENALTY = 0.5  # Reduce qty by 50% for zones that flip roles\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Will process top {TOP_N} stations into ~{math.ceil(TOP_N/TARGET_STATIONS_PER_ZONE)} zones\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD AND PREPARE DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Loading data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Parse datetime columns\n",
    "for col in [\"started_at\", \"ended_at\", \"date\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "print(f\"Loaded {len(df):,} trip records\")\n",
    "print(f\"Date range: {df['started_at'].min()} to {df['started_at'].max()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: IDENTIFY TOP STATIONS AND CREATE GEOGRAPHIC ZONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Identifying top stations and creating geographic zones...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get station coordinates (prefer start coords, fallback to end coords)\n",
    "starts_geo = (df.dropna(subset=[\"start_station_name\", \"start_lat\", \"start_lng\"])\n",
    "              .groupby(\"start_station_name\")[[\"start_lat\", \"start_lng\"]]\n",
    "              .median()\n",
    "              .rename(columns={\"start_lat\": \"lat\", \"start_lng\": \"lon\"}))\n",
    "\n",
    "ends_geo = (df.dropna(subset=[\"end_station_name\", \"end_lat\", \"end_lng\"])\n",
    "            .groupby(\"end_station_name\")[[\"end_lat\", \"end_lng\"]]\n",
    "            .median()\n",
    "            .rename(columns={\"end_lat\": \"lat\", \"end_lng\": \"lon\"}))\n",
    "\n",
    "stations_all = starts_geo.combine_first(ends_geo)\n",
    "stations_all.index.name = \"station\"\n",
    "\n",
    "# Calculate total volume per station\n",
    "starts_count = df.groupby(\"start_station_name\").size().rename(\"starts\")\n",
    "ends_count = df.groupby(\"end_station_name\").size().rename(\"ends\")\n",
    "\n",
    "station_volume = (pd.concat([starts_count, ends_count], axis=1)\n",
    "                  .fillna(0)\n",
    "                  .assign(total=lambda x: x.starts + x.ends)\n",
    "                  .sort_values(\"total\", ascending=False))\n",
    "\n",
    "# Keep top N stations\n",
    "top_stations = (station_volume.head(TOP_N)\n",
    "                .join(stations_all, how=\"left\")\n",
    "                .dropna(subset=[\"lat\", \"lon\"])\n",
    "                .copy())\n",
    "\n",
    "print(f\"Selected top {len(top_stations)} stations\")\n",
    "print(f\"Total trips covered: {top_stations['total'].sum():,.0f} ({top_stations['total'].sum()/len(df)*100:.1f}% of all trips)\")\n",
    "\n",
    "# Cluster into geographic zones using K-means\n",
    "n_clusters = max(1, math.ceil(len(top_stations) / TARGET_STATIONS_PER_ZONE))\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                         batch_size=256, n_init=\"auto\")\n",
    "\n",
    "top_stations[\"geo_zone\"] = kmeans.fit_predict(top_stations[[\"lat\", \"lon\"]])\n",
    "\n",
    "# Remap zone IDs to sequential integers\n",
    "zone_mapping = {old: new for new, old in enumerate(sorted(top_stations[\"geo_zone\"].unique()))}\n",
    "top_stations[\"geo_zone\"] = top_stations[\"geo_zone\"].map(zone_mapping).astype(int)\n",
    "\n",
    "print(f\"Created {top_stations['geo_zone'].nunique()} geographic zones\")\n",
    "print(f\"Average stations per zone: {len(top_stations) / top_stations['geo_zone'].nunique():.1f}\")\n",
    "\n",
    "# Create lookup dictionaries\n",
    "station_to_zone = top_stations[\"geo_zone\"].to_dict()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: CALCULATE HOURLY FLOWS PER ZONE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Calculating hourly flows per zone...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Trips STARTING from each zone (departures)\n",
    "starts_hourly = (df.loc[df[\"start_station_name\"].isin(top_stations.index)]\n",
    "                 .dropna(subset=[\"started_at\"])\n",
    "                 .assign(zone=lambda x: x[\"start_station_name\"].map(station_to_zone),\n",
    "                        hour=lambda x: x[\"started_at\"].dt.hour,\n",
    "                        dow=lambda x: x[\"started_at\"].dt.dayofweek,\n",
    "                        month=lambda x: x[\"started_at\"].dt.to_period(\"M\").astype(str))\n",
    "                 .groupby([\"month\", \"dow\", \"hour\", \"zone\"])\n",
    "                 .size()\n",
    "                 .rename(\"starts\")\n",
    "                 .reset_index())\n",
    "\n",
    "# Trips ENDING at each zone (arrivals)\n",
    "ends_hourly = (df.loc[df[\"end_station_name\"].isin(top_stations.index)]\n",
    "               .dropna(subset=[\"ended_at\"])\n",
    "               .assign(zone=lambda x: x[\"end_station_name\"].map(station_to_zone),\n",
    "                      hour=lambda x: x[\"ended_at\"].dt.hour,\n",
    "                      dow=lambda x: x[\"ended_at\"].dt.dayofweek,\n",
    "                      month=lambda x: x[\"ended_at\"].dt.to_period(\"M\").astype(str))\n",
    "               .groupby([\"month\", \"dow\", \"hour\", \"zone\"])\n",
    "               .size()\n",
    "               .rename(\"ends\")\n",
    "               .reset_index())\n",
    "\n",
    "# Create complete grid (all month×dow×hour×zone combinations)\n",
    "all_months = sorted(set(starts_hourly[\"month\"]) | set(ends_hourly[\"month\"]))\n",
    "all_dows = sorted(set(starts_hourly[\"dow\"]) | set(ends_hourly[\"dow\"]))\n",
    "all_hours = range(24)\n",
    "all_zones = sorted(top_stations[\"geo_zone\"].unique())\n",
    "\n",
    "grid = pd.MultiIndex.from_product(\n",
    "    [all_months, all_dows, all_hours, all_zones],\n",
    "    names=[\"month\", \"dow\", \"hour\", \"zone\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Merge and fill missing values with 0\n",
    "baseline_hourly = (grid.merge(starts_hourly, on=[\"month\", \"dow\", \"hour\", \"zone\"], how=\"left\")\n",
    "                   .merge(ends_hourly, on=[\"month\", \"dow\", \"hour\", \"zone\"], how=\"left\")\n",
    "                   .fillna({\"starts\": 0, \"ends\": 0}))\n",
    "\n",
    "# Calculate net flow: C = ends - starts\n",
    "# Positive C = bikes accumulating (supply/sink)\n",
    "# Negative C = bikes depleting (demand/source)\n",
    "baseline_hourly[\"C\"] = baseline_hourly[\"ends\"] - baseline_hourly[\"starts\"]\n",
    "\n",
    "# Smooth C to reduce noise\n",
    "baseline_hourly[\"C_smooth\"] = (baseline_hourly\n",
    "    .sort_values([\"month\", \"dow\", \"zone\", \"hour\"])\n",
    "    .groupby([\"month\", \"dow\", \"zone\"])[\"C\"]\n",
    "    .transform(lambda x: x.rolling(SMOOTH_WINDOW, center=True, min_periods=1).mean()))\n",
    "\n",
    "print(f\"Created baseline with {len(baseline_hourly):,} records\")\n",
    "print(f\"Coverage: {len(all_months)} months × {len(all_dows)} days × 24 hours × {len(all_zones)} zones\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: DETECT TIME BLOCKS PER ZONE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Detecting time blocks per zone...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def detect_blocks_for_zone(df_zone):\n",
    "    \"\"\"\n",
    "    Detect time blocks for one zone's 24-hour pattern.\n",
    "    Returns list of dicts with hour_from, hour_to, C_expected, action.\n",
    "    \"\"\"\n",
    "    df_zone = df_zone.sort_values(\"hour\").reset_index(drop=True)\n",
    "    hours = df_zone[\"hour\"].to_numpy()\n",
    "    C = df_zone[\"C\"].to_numpy()\n",
    "    C_smooth = df_zone[\"C_smooth\"].to_numpy()\n",
    "    \n",
    "    # Classify hours by sign (with deadband)\n",
    "    sign = np.where(C_smooth > EPS, 1, \n",
    "                    np.where(C_smooth < -EPS, -1, 0))\n",
    "    \n",
    "    blocks = []\n",
    "    i = 0\n",
    "    while i < 24:\n",
    "        if sign[i] == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Find continuous run of same sign\n",
    "        current_sign = sign[i]\n",
    "        j = i\n",
    "        while j < 24 and sign[j] == current_sign:\n",
    "            j += 1\n",
    "        \n",
    "        # Calculate total C over this period\n",
    "        total_C = float(C[i:j].sum())\n",
    "        duration = j - i\n",
    "        \n",
    "        # Keep block if it meets thresholds\n",
    "        if duration >= MIN_BLOCK_HOURS and abs(total_C) >= MIN_ABS_C:\n",
    "            blocks.append({\n",
    "                \"hour_from\": int(hours[i]),\n",
    "                \"hour_to\": int(hours[j-1] + 1),  # Exclusive end\n",
    "                \"C_expected\": total_C,\n",
    "                \"action\": \"supply\" if total_C > 0 else \"demand\"\n",
    "            })\n",
    "        \n",
    "        i = j\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "# Detect blocks for each zone×month×dow\n",
    "rows = []\n",
    "for (m, d, z), group in baseline_hourly.groupby([\"month\", \"dow\", \"zone\"], sort=False):\n",
    "    for block in detect_blocks_for_zone(group):\n",
    "        rows.append({\n",
    "            \"month\": m,\n",
    "            \"dow\": int(d),\n",
    "            \"zone\": int(z),\n",
    "            **block\n",
    "        })\n",
    "\n",
    "moves_blocks_raw = pd.DataFrame(rows).sort_values([\"month\", \"dow\", \"hour_from\", \"zone\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Detected {len(moves_blocks_raw)} raw blocks\")\n",
    "print(f\"Average blocks per zone×month×dow: {len(moves_blocks_raw) / (len(all_months) * len(all_dows) * len(all_zones)):.1f}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample blocks:\")\n",
    "print(moves_blocks_raw.head(10))\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CONSOLIDATE SIMILAR BLOCKS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Consolidating similar blocks...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "blocks_catalog_raw = (moves_blocks_raw\n",
    "    .drop(columns=[\"zone\", \"C_expected\", \"action\"])\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"month\", \"dow\", \"hour_from\"])\n",
    "    .assign(block=lambda x: x[\"hour_from\"].map(\"{:02d}\".format) + \":00–\" + \n",
    "                            x[\"hour_to\"].map(\"{:02d}\".format) + \":00\"))\n",
    "\n",
    "print(f\"Unique blocks BEFORE consolidation: {len(blocks_catalog_raw)}\")\n",
    "\n",
    "def create_overnight_blocks(baseline_df, overnight_start=23, overnight_end=4, min_abs_c=6):\n",
    "    \"\"\"\n",
    "    Create canonical overnight blocks directly from hourly data.\n",
    "    For each month×dow×zone, sum C over overnight hours.\n",
    "    \"\"\"\n",
    "    overnight_rows = []\n",
    "    \n",
    "    # Define overnight hours (wraps around midnight: 23, 0, 1, 2, 3)\n",
    "    overnight_hours = [h for h in range(overnight_start, 24)] + [h for h in range(0, overnight_end)]\n",
    "    \n",
    "    for (m, d, z), group in baseline_df.groupby(['month', 'dow', 'zone']):\n",
    "        # Get only overnight hours\n",
    "        overnight_data = group[group['hour'].isin(overnight_hours)]\n",
    "        \n",
    "        if overnight_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Sum C over overnight window\n",
    "        total_C = overnight_data['C'].sum()\n",
    "        \n",
    "        # Only include if significant\n",
    "        if abs(total_C) >= min_abs_c:\n",
    "            overnight_rows.append({\n",
    "                'month': m,\n",
    "                'dow': d,\n",
    "                'zone': z,\n",
    "                'hour_from': overnight_start,\n",
    "                'hour_to': overnight_end,\n",
    "                'C_expected': total_C,\n",
    "                'action': 'supply' if total_C > 0 else 'demand'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(overnight_rows)\n",
    "\n",
    "def filter_daytime_blocks(moves_blocks_df, overnight_start=23, overnight_end=4):\n",
    "    \"\"\"\n",
    "    Remove any detected blocks that fall within overnight hours.\n",
    "    Keep only blocks that are strictly in daytime (04:00-23:00).\n",
    "    \"\"\"\n",
    "    # A block is daytime if it starts at or after overnight_end AND ends before overnight_start\n",
    "    daytime_mask = (moves_blocks_df['hour_from'] >= overnight_end) & \\\n",
    "                   (moves_blocks_df['hour_to'] <= overnight_start)\n",
    "    \n",
    "    return moves_blocks_df[daytime_mask].copy()\n",
    "\n",
    "def consolidate_daytime_blocks(daytime_df, tolerance_hours=3):\n",
    "    \"\"\"\n",
    "    Consolidate daytime blocks using tolerance-based merging.\n",
    "    \"\"\"\n",
    "    consolidated = []\n",
    "    \n",
    "    for (m, d), group in daytime_df.groupby(['month', 'dow']):\n",
    "        boundaries = group[['hour_from', 'hour_to']].drop_duplicates().sort_values('hour_from')\n",
    "        \n",
    "        canonical_blocks = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for idx, row in boundaries.iterrows():\n",
    "            if idx in used_indices:\n",
    "                continue\n",
    "            \n",
    "            similar_mask = (\n",
    "                (abs(boundaries['hour_from'] - row['hour_from']) <= tolerance_hours) &\n",
    "                (abs(boundaries['hour_to'] - row['hour_to']) <= tolerance_hours)\n",
    "            )\n",
    "            similar = boundaries[similar_mask]\n",
    "            \n",
    "            canon_from = int(similar['hour_from'].median())\n",
    "            canon_to = int(similar['hour_to'].median())\n",
    "            \n",
    "            canonical_blocks.append((canon_from, canon_to))\n",
    "            used_indices.update(similar.index)\n",
    "        \n",
    "        # Assign zones to canonical blocks\n",
    "        for canon_from, canon_to in canonical_blocks:\n",
    "            matching_zones = group[\n",
    "                (abs(group['hour_from'] - canon_from) <= tolerance_hours) &\n",
    "                (abs(group['hour_to'] - canon_to) <= tolerance_hours)\n",
    "            ]\n",
    "            \n",
    "            for _, zone_row in matching_zones.iterrows():\n",
    "                consolidated.append({\n",
    "                    'month': m,\n",
    "                    'dow': d,\n",
    "                    'zone': zone_row['zone'],\n",
    "                    'hour_from': canon_from,\n",
    "                    'hour_to': canon_to,\n",
    "                    'C_expected': zone_row['C_expected'],\n",
    "                    'action': zone_row['action']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(consolidated)\n",
    "\n",
    "# Execute the two-regime approach\n",
    "if FORCE_OVERNIGHT_BLOCK:\n",
    "    print(f\"\\nCreating canonical overnight blocks ({OVERNIGHT_START:02d}:00-{OVERNIGHT_END:02d}:00)...\")\n",
    "    overnight_blocks = create_overnight_blocks(baseline_hourly, \n",
    "                                               overnight_start=OVERNIGHT_START,\n",
    "                                               overnight_end=OVERNIGHT_END,\n",
    "                                               min_abs_c=MIN_ABS_C)\n",
    "    print(f\"  Generated {len(overnight_blocks)} overnight zone assignments\")\n",
    "    \n",
    "    print(f\"\\nFiltering daytime blocks ({OVERNIGHT_END:02d}:00-{OVERNIGHT_START:02d}:00)...\")\n",
    "    daytime_blocks = filter_daytime_blocks(moves_blocks_raw,\n",
    "                                           overnight_start=OVERNIGHT_START,\n",
    "                                           overnight_end=OVERNIGHT_END)\n",
    "    print(f\"  Kept {len(daytime_blocks)} daytime blocks from raw detection\")\n",
    "    \n",
    "    print(f\"\\nConsolidating daytime blocks (tolerance={CONSOLIDATION_TOLERANCE}h)...\")\n",
    "    consolidated_daytime = consolidate_daytime_blocks(daytime_blocks,\n",
    "                                                      tolerance_hours=CONSOLIDATION_TOLERANCE)\n",
    "    print(f\"  Consolidated to {len(consolidated_daytime)} daytime zone assignments\")\n",
    "    \n",
    "    # Combine overnight + daytime\n",
    "    moves_blocks = pd.concat([overnight_blocks, consolidated_daytime], ignore_index=True)\n",
    "    moves_blocks = moves_blocks.sort_values(['month', 'dow', 'hour_from', 'zone']).reset_index(drop=True)\n",
    "else:\n",
    "    # No overnight enforcement, just consolidate everything\n",
    "    moves_blocks = consolidate_daytime_blocks(moves_blocks_raw, \n",
    "                                              tolerance_hours=CONSOLIDATION_TOLERANCE)\n",
    "\n",
    "# Create catalog\n",
    "blocks_catalog = (moves_blocks\n",
    "    .drop(columns=[\"zone\", \"C_expected\", \"action\"])\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"month\", \"dow\", \"hour_from\"])\n",
    "    .assign(block=lambda x: x[\"hour_from\"].map(\"{:02d}\".format) + \":00–\" + \n",
    "                            x[\"hour_to\"].map(\"{:02d}\".format) + \":00\"))\n",
    "\n",
    "print(f\"\\nUnique blocks AFTER consolidation: {len(blocks_catalog)}\")\n",
    "print(f\"Reduction: {len(blocks_catalog_raw)} → {len(blocks_catalog)} ({(1 - len(blocks_catalog)/len(blocks_catalog_raw))*100:.1f}% reduction)\")\n",
    "\n",
    "# Verify overnight blocks\n",
    "if FORCE_OVERNIGHT_BLOCK:\n",
    "    overnight_catalog = blocks_catalog[(blocks_catalog['hour_from'] == OVERNIGHT_START) & \n",
    "                                       (blocks_catalog['hour_to'] == OVERNIGHT_END)]\n",
    "    print(f\"\\nOvernight blocks ({OVERNIGHT_START:02d}:00-{OVERNIGHT_END:02d}:00): {len(overnight_catalog)} found\")\n",
    "    if len(overnight_catalog) > 0:\n",
    "        print(\"✓ Canonical overnight block successfully enforced\")\n",
    "\n",
    "print(\"\\nBlocks per month×dow after consolidation (sample):\")\n",
    "sample_blocks = blocks_catalog.groupby([\"month\", \"dow\"])[\"block\"].apply(list).head()\n",
    "for (m, d), block_list in sample_blocks.items():\n",
    "    print(f\"{m} dow={d}: {len(block_list)} blocks - {block_list}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPOSE KEY OBJECTS FOR NEXT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "stations_top = top_stations.rename_axis(\"station\").reset_index()[[\"station\", \"lat\", \"lon\", \"geo_zone\"]]\n",
    "# moves_blocks and blocks_catalog already updated by consolidation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey objects created:\")\n",
    "print(f\"- stations_top: {len(stations_top)} stations\")\n",
    "print(f\"- baseline_hourly: {len(baseline_hourly)} records\")\n",
    "print(f\"- moves_blocks: {len(moves_blocks)} blocks (after consolidation)\")\n",
    "print(f\"- blocks_catalog: {len(blocks_catalog)} unique time windows (after consolidation)\")\n",
    "print(f\"\\nAverage blocks per month×dow: {len(blocks_catalog) / (len(all_months) * len(all_dows)):.1f}\")\n",
    "print(\"\\nReady for next steps:\")\n",
    "print(\"1. ✓ Block consolidation (COMPLETE)\")\n",
    "print(\"2. Day-level zone profiling\")\n",
    "print(\"3. Bidirectional demand adjustment\")\n",
    "print(\"4. Sticky zones heuristic\")\n",
    "print(\"5. Interactive map\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: DAY-LEVEL ZONE PROFILING (Self-Correcting Zones)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Day-level zone profiling (identify self-correcting zones)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def identify_self_correcting_zones(moves_blocks_df, \n",
    "                                    morning_threshold=-20,\n",
    "                                    evening_threshold=20,\n",
    "                                    midday_threshold=15):\n",
    "    \"\"\"\n",
    "    Identify residential zones with natural daily rhythms that don't need intervention.\n",
    "    \n",
    "    Criteria:\n",
    "    - Morning deficit (bikes flow out): sum(C) < morning_threshold\n",
    "    - Evening surplus (bikes return): sum(C) > evening_threshold\n",
    "    - Low midday activity: max(|C|) < midday_threshold\n",
    "    \n",
    "    These zones self-correct over the day and should be excluded from recommendations.\n",
    "    \"\"\"\n",
    "    self_correcting = []\n",
    "    \n",
    "    for (m, d, z), group in moves_blocks_df.groupby(['month', 'dow', 'zone']):\n",
    "        # Exclude overnight block from this analysis\n",
    "        daytime_blocks = group[~((group['hour_from'] == OVERNIGHT_START) & \n",
    "                                  (group['hour_to'] == OVERNIGHT_END))]\n",
    "        \n",
    "        if daytime_blocks.empty:\n",
    "            continue\n",
    "        \n",
    "        # Morning blocks: ending before 12:00\n",
    "        morning_blocks = daytime_blocks[daytime_blocks['hour_to'] <= 12]\n",
    "        morning_C = morning_blocks['C_expected'].sum() if not morning_blocks.empty else 0\n",
    "        \n",
    "        # Evening blocks: starting after 16:00\n",
    "        evening_blocks = daytime_blocks[daytime_blocks['hour_from'] >= 16]\n",
    "        evening_C = evening_blocks['C_expected'].sum() if not evening_blocks.empty else 0\n",
    "        \n",
    "        # Midday blocks: between 10:00 and 16:00\n",
    "        midday_blocks = daytime_blocks[(daytime_blocks['hour_from'] >= 10) & \n",
    "                                        (daytime_blocks['hour_to'] <= 16)]\n",
    "        midday_max = midday_blocks['C_expected'].abs().max() if not midday_blocks.empty else 0\n",
    "        \n",
    "        # Check residential pattern\n",
    "        is_residential = (morning_C < morning_threshold and \n",
    "                         evening_C > evening_threshold and \n",
    "                         midday_max < midday_threshold)\n",
    "        \n",
    "        if is_residential:\n",
    "            self_correcting.append({\n",
    "                'month': m,\n",
    "                'dow': d,\n",
    "                'zone': z,\n",
    "                'morning_C': morning_C,\n",
    "                'evening_C': evening_C,\n",
    "                'midday_max': midday_max,\n",
    "                'profile': 'residential_self_correcting'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(self_correcting)\n",
    "\n",
    "# Identify self-correcting zones\n",
    "self_correcting_zones = identify_self_correcting_zones(\n",
    "    moves_blocks,\n",
    "    morning_threshold=RESIDENTIAL_MORNING_THRESHOLD,\n",
    "    evening_threshold=RESIDENTIAL_EVENING_THRESHOLD,\n",
    "    midday_threshold=RESIDENTIAL_MIDDAY_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"Found {len(self_correcting_zones)} self-correcting zone×month×dow combinations\")\n",
    "print(f\"Unique zones flagged: {self_correcting_zones['zone'].nunique()} out of {len(all_zones)}\")\n",
    "\n",
    "if not self_correcting_zones.empty:\n",
    "    print(\"\\nSample self-correcting zones:\")\n",
    "    print(self_correcting_zones.head(10)[['month', 'dow', 'zone', 'morning_C', 'evening_C', 'midday_max']])\n",
    "    \n",
    "    # Filter moves_blocks to exclude self-correcting zones\n",
    "    merge_key = ['month', 'dow', 'zone']\n",
    "    moves_blocks_filtered = moves_blocks.merge(\n",
    "        self_correcting_zones[merge_key + ['profile']],\n",
    "        on=merge_key,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    excluded_count = moves_blocks_filtered['profile'].notna().sum()\n",
    "    moves_blocks = moves_blocks_filtered[moves_blocks_filtered['profile'].isna()].drop(columns=['profile'])\n",
    "    \n",
    "    print(f\"\\nExcluded {excluded_count} block assignments from self-correcting zones\")\n",
    "    print(f\"Remaining blocks: {len(moves_blocks)}\")\n",
    "    \n",
    "    # Update catalog\n",
    "    blocks_catalog = (moves_blocks\n",
    "        .drop(columns=[\"zone\", \"C_expected\", \"action\"])\n",
    "        .drop_duplicates()\n",
    "        .sort_values([\"month\", \"dow\", \"hour_from\"])\n",
    "        .assign(block=lambda x: x[\"hour_from\"].map(\"{:02d}\".format) + \":00–\" + \n",
    "                                x[\"hour_to\"].map(\"{:02d}\".format) + \":00\"))\n",
    "    \n",
    "    print(f\"Updated catalog: {len(blocks_catalog)} unique time windows\")\n",
    "else:\n",
    "    print(\"No self-correcting zones found with current thresholds\")\n",
    "\n",
    "print(\"\\n✓ Day-level zone profiling COMPLETE\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: BIDIRECTIONAL DEMAND ADJUSTMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: Bidirectional demand adjustment...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def add_bidirectional_metrics(moves_blocks_df, baseline_df):\n",
    "    \"\"\"\n",
    "    Add starts/ends data to each block and calculate rental/return pressure.\n",
    "    This prevents stripping busy stations of all bikes just because net flow is positive.\n",
    "    \"\"\"\n",
    "    enriched = []\n",
    "    \n",
    "    for _, row in moves_blocks_df.iterrows():\n",
    "        m, d, z = row['month'], row['dow'], row['zone']\n",
    "        hf, ht = row['hour_from'], row['hour_to']\n",
    "        \n",
    "        # Get hourly data for this zone×block window\n",
    "        window_data = baseline_df[\n",
    "            (baseline_df['month'] == m) &\n",
    "            (baseline_df['dow'] == d) &\n",
    "            (baseline_df['zone'] == z) &\n",
    "            (baseline_df['hour'] >= hf) &\n",
    "            (baseline_df['hour'] < ht)\n",
    "        ]\n",
    "        \n",
    "        if window_data.empty:\n",
    "            # Shouldn't happen, but handle gracefully\n",
    "            total_starts = 0\n",
    "            total_ends = 0\n",
    "        else:\n",
    "            total_starts = window_data['starts'].sum()\n",
    "            total_ends = window_data['ends'].sum()\n",
    "        \n",
    "        total_activity = total_starts + total_ends\n",
    "        \n",
    "        # Calculate pressure ratios (avoid division by zero)\n",
    "        if total_activity > 0:\n",
    "            rental_pressure = total_starts / total_activity\n",
    "            return_pressure = total_ends / total_activity\n",
    "        else:\n",
    "            rental_pressure = 0.5\n",
    "            return_pressure = 0.5\n",
    "        \n",
    "        enriched.append({\n",
    "            'month': m,\n",
    "            'dow': d,\n",
    "            'zone': z,\n",
    "            'hour_from': hf,\n",
    "            'hour_to': ht,\n",
    "            'C_expected': row['C_expected'],\n",
    "            'action': row['action'],\n",
    "            'starts': total_starts,\n",
    "            'ends': total_ends,\n",
    "            'total_activity': total_activity,\n",
    "            'rental_pressure': rental_pressure,\n",
    "            'return_pressure': return_pressure\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(enriched)\n",
    "\n",
    "# Enrich moves_blocks with bidirectional metrics\n",
    "print(\"Adding starts/ends data from baseline_hourly...\")\n",
    "moves_blocks_enriched = add_bidirectional_metrics(moves_blocks, baseline_hourly)\n",
    "\n",
    "print(f\"Enriched {len(moves_blocks_enriched)} block assignments\")\n",
    "print(\"\\nSample enriched data:\")\n",
    "print(moves_blocks_enriched.head(10)[['month', 'dow', 'zone', 'hour_from', 'hour_to', \n",
    "                                       'C_expected', 'starts', 'ends', 'rental_pressure', 'return_pressure']])\n",
    "\n",
    "# Calculate adjusted movable quantities\n",
    "def calculate_movable_quantity(C_expected, rental_pressure, return_pressure, action):\n",
    "    \"\"\"\n",
    "    Scale movable bikes based on bidirectional pressure.\n",
    "    \n",
    "    For supply zones (C > 0): Use return_pressure\n",
    "      Logic: Zone has X% returns, so only X% of surplus is truly excess\n",
    "    \n",
    "    For demand zones (C < 0): Use rental_pressure\n",
    "      Logic: Zone has X% rentals, so only X% of shortage is critical\n",
    "    \"\"\"\n",
    "    if action == 'supply':\n",
    "        # Surplus bikes, but station needs capacity for ongoing rentals\n",
    "        return abs(C_expected) * return_pressure\n",
    "    else:  # demand\n",
    "        # Shortage, but station also has ongoing returns\n",
    "        return abs(C_expected) * rental_pressure\n",
    "\n",
    "moves_blocks_enriched['movable_raw'] = moves_blocks_enriched.apply(\n",
    "    lambda r: calculate_movable_quantity(r['C_expected'], r['rental_pressure'], \n",
    "                                         r['return_pressure'], r['action']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nMovable quantities calculated (before β scaling)\")\n",
    "print(f\"Average adjustment factor: {(moves_blocks_enriched['movable_raw'] / moves_blocks_enriched['C_expected'].abs()).mean():.2f}\")\n",
    "\n",
    "# Show examples of adjustment impact\n",
    "print(\"\\nExamples of bidirectional adjustment:\")\n",
    "sample = moves_blocks_enriched[moves_blocks_enriched['total_activity'] > 100].head(5)\n",
    "for _, r in sample.iterrows():\n",
    "    adjustment_pct = (r['movable_raw'] / abs(r['C_expected'])) * 100\n",
    "    print(f\"  Zone {r['zone']}, {r['action']}: C={r['C_expected']:.0f}, \"\n",
    "          f\"starts={r['starts']:.0f}, ends={r['ends']:.0f} → \"\n",
    "          f\"movable={r['movable_raw']:.0f} ({adjustment_pct:.0f}% of |C|)\")\n",
    "\n",
    "# Replace moves_blocks with enriched version\n",
    "moves_blocks = moves_blocks_enriched.copy()\n",
    "\n",
    "print(\"\\n✓ Bidirectional demand adjustment COMPLETE\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: STICKY ZONES HEURISTIC (Forward-Looking Hedge)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: Sticky zones heuristic (reduce moves for role-flipping zones)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def identify_sticky_zones(moves_blocks_df, flip_penalty=0.5):\n",
    "    \"\"\"\n",
    "    Identify zones that flip roles between consecutive blocks.\n",
    "    Reduce their movable quantity to avoid wasteful back-and-forth moves.\n",
    "    \n",
    "    A zone \"flips\" if it's supply in block N and demand in block N+1 (or vice versa).\n",
    "    \"\"\"\n",
    "    moves_blocks_df = moves_blocks_df.sort_values(['month', 'dow', 'zone', 'hour_from']).reset_index(drop=True)\n",
    "    \n",
    "    sticky_flags = []\n",
    "    \n",
    "    for (m, d, z), group in moves_blocks_df.groupby(['month', 'dow', 'zone']):\n",
    "        group = group.sort_values('hour_from').reset_index(drop=True)\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            is_sticky = False\n",
    "            \n",
    "            # Check if there's a next block for this zone\n",
    "            if i < len(group) - 1:\n",
    "                current_action = group.iloc[i]['action']\n",
    "                next_action = group.iloc[i + 1]['action']\n",
    "                \n",
    "                # Zone flips if actions differ\n",
    "                if current_action != next_action:\n",
    "                    is_sticky = True\n",
    "            \n",
    "            sticky_flags.append(is_sticky)\n",
    "    \n",
    "    moves_blocks_df['is_sticky'] = sticky_flags\n",
    "    \n",
    "    # Apply penalty to sticky zones\n",
    "    moves_blocks_df['movable_adjusted'] = moves_blocks_df.apply(\n",
    "        lambda r: r['movable_raw'] * flip_penalty if r['is_sticky'] else r['movable_raw'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return moves_blocks_df\n",
    "\n",
    "# Apply sticky zones logic\n",
    "print(\"Checking for zones that flip roles between consecutive blocks...\")\n",
    "moves_blocks = identify_sticky_zones(moves_blocks, flip_penalty=FLIP_PENALTY)\n",
    "\n",
    "sticky_count = moves_blocks['is_sticky'].sum()\n",
    "total_count = len(moves_blocks)\n",
    "sticky_pct = (sticky_count / total_count) * 100\n",
    "\n",
    "print(f\"Found {sticky_count} sticky blocks out of {total_count} ({sticky_pct:.1f}%)\")\n",
    "print(f\"These will have movable quantities reduced by {(1-FLIP_PENALTY)*100:.0f}%\")\n",
    "\n",
    "if sticky_count > 0:\n",
    "    print(\"\\nSample sticky zones (role-flippers):\")\n",
    "    sticky_sample = moves_blocks[moves_blocks['is_sticky']].head(10)\n",
    "    for _, r in sticky_sample.iterrows():\n",
    "        print(f\"  Zone {r['zone']}, {r['month']} dow={r['dow']}, \"\n",
    "              f\"{r['hour_from']:02d}:00-{r['hour_to']:02d}:00 ({r['action']}): \"\n",
    "              f\"movable {r['movable_raw']:.0f} → {r['movable_adjusted']:.0f}\")\n",
    "\n",
    "print(\"\\n✓ Sticky zones heuristic COMPLETE\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL BUILD COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFinal statistics:\")\n",
    "print(f\"- Stations: {len(stations_top)}\")\n",
    "print(f\"- Zones: {len(all_zones)}\")\n",
    "print(f\"- Time blocks: {len(blocks_catalog)} unique windows\")\n",
    "print(f\"- Block assignments: {len(moves_blocks)} (after all filtering)\")\n",
    "print(f\"- Average blocks per month×dow: {len(blocks_catalog) / (len(all_months) * len(all_dows)):.1f}\")\n",
    "\n",
    "print(\"\\nFeatures implemented:\")\n",
    "print(\"✓ Geographic zone clustering\")\n",
    "print(\"✓ Zone-level block detection\")\n",
    "print(\"✓ Block consolidation (tolerance=3h)\")\n",
    "print(\"✓ Canonical overnight block (23:00-04:00)\")\n",
    "print(\"✓ Day-level zone profiling (self-correcting zones)\")\n",
    "print(\"✓ Bidirectional demand adjustment\")\n",
    "print(\"✓ Sticky zones heuristic (role-flippers)\")\n",
    "\n",
    "print(\"\\nReady for:\")\n",
    "print(\"- Interactive map visualization\")\n",
    "print(\"- User-controlled β parameter for intervention scaling\")\n",
    "print(\"- Greedy matching algorithm for supply→demand routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e19918-916e-469a-9646-da248e08561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f515d003ea46c9b7047b6773bc53d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Month:', options=('2022-01', '2022-02', '2022-03', '2022-0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccaa69b4f83491ea3917b397377d4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INTERACTIVE MAP READY\n",
      "================================================================================\n",
      "\n",
      "Controls:\n",
      "- Month/Day/Time Block: Select when to view\n",
      "- β (beta): Fraction of movable bikes to actually move (0.1=conservative, 0.9=aggressive)\n",
      "- Min bikes: Hide zones with recommendations below this threshold\n",
      "- Max routes: Limit number of supply→demand routes shown\n",
      "\n",
      "Colors:\n",
      "- Green zones = Supply (excess bikes)\n",
      "- Red zones = Demand (need bikes)\n",
      "- Lines = Suggested redistribution routes (thickness = quantity)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INTERACTIVE REBALANCING MAP\n",
    "# =============================================================================\n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely import concave_hull\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate distance between two points in kilometers.\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c  # Earth radius in km\n",
    "\n",
    "def greedy_matching(supply_zones, demand_zones, top_k=15):\n",
    "    \"\"\"\n",
    "    Greedy nearest-neighbor matching between supply and demand zones.\n",
    "    Returns list of (supply_zone, demand_zone, bikes_moved) tuples.\n",
    "    \"\"\"\n",
    "    if supply_zones.empty or demand_zones.empty:\n",
    "        return pd.DataFrame(columns=['supply_zone', 'demand_zone', 'bikes_moved'])\n",
    "    \n",
    "    supply = supply_zones.copy()\n",
    "    demand = demand_zones.copy()\n",
    "    \n",
    "    routes = []\n",
    "    \n",
    "    while not supply.empty and not demand.empty and supply['qty'].sum() > 0 and demand['qty'].sum() > 0:\n",
    "        # Find closest supply-demand pair\n",
    "        min_dist = float('inf')\n",
    "        best_s_idx = None\n",
    "        best_d_idx = None\n",
    "        \n",
    "        for s_idx, s_row in supply.iterrows():\n",
    "            if s_row['qty'] <= 0:\n",
    "                continue\n",
    "            for d_idx, d_row in demand.iterrows():\n",
    "                if d_row['qty'] <= 0:\n",
    "                    continue\n",
    "                dist = haversine_distance(s_row['lat'], s_row['lon'], \n",
    "                                         d_row['lat'], d_row['lon'])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_s_idx = s_idx\n",
    "                    best_d_idx = d_idx\n",
    "        \n",
    "        if best_s_idx is None or best_d_idx is None:\n",
    "            break\n",
    "        \n",
    "        # Move bikes\n",
    "        bikes_to_move = min(supply.loc[best_s_idx, 'qty'], \n",
    "                           demand.loc[best_d_idx, 'qty'])\n",
    "        \n",
    "        routes.append({\n",
    "            'supply_zone': supply.loc[best_s_idx, 'zone'],\n",
    "            'demand_zone': demand.loc[best_d_idx, 'zone'],\n",
    "            'bikes_moved': bikes_to_move\n",
    "        })\n",
    "        \n",
    "        supply.loc[best_s_idx, 'qty'] -= bikes_to_move\n",
    "        demand.loc[best_d_idx, 'qty'] -= bikes_to_move\n",
    "        \n",
    "        if len(routes) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(routes).sort_values('bikes_moved', ascending=False)\n",
    "\n",
    "def get_zone_centroids(stations_df):\n",
    "    \"\"\"Calculate centroid for each zone.\"\"\"\n",
    "    return stations_df.groupby('geo_zone')[['lat', 'lon']].mean().reset_index().rename(columns={'geo_zone': 'zone'})\n",
    "\n",
    "def create_zone_hulls_geojson(stations_df, zones_to_show):\n",
    "    \"\"\"Create convex hulls for zones.\"\"\"\n",
    "    features = []\n",
    "    for zone in zones_to_show:\n",
    "        zone_stations = stations_df[stations_df['geo_zone'] == zone]\n",
    "        if len(zone_stations) >= 3:\n",
    "            points = [(row['lon'], row['lat']) for _, row in zone_stations.iterrows()]\n",
    "            mp = MultiPoint(points)\n",
    "            hull = mp.convex_hull\n",
    "            features.append({\n",
    "                'type': 'Feature',\n",
    "                'properties': {'zone': int(zone)},\n",
    "                'geometry': hull.__geo_interface__\n",
    "            })\n",
    "    return {'type': 'FeatureCollection', 'features': features}\n",
    "\n",
    "def format_bikes(n):\n",
    "    \"\"\"Format bike count for display.\"\"\"\n",
    "    return f\"{n/1000:.1f}k\" if n >= 1000 else str(int(round(n)))\n",
    "\n",
    "# =============================================================================\n",
    "# WIDGET SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Get unique months and days\n",
    "available_months = sorted(blocks_catalog['month'].unique())\n",
    "available_dows = sorted(blocks_catalog['dow'].unique())\n",
    "dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# Create widgets\n",
    "w_month = widgets.Dropdown(\n",
    "    options=available_months,\n",
    "    value=available_months[0],\n",
    "    description='Month:'\n",
    ")\n",
    "\n",
    "w_dow = widgets.Dropdown(\n",
    "    options=[(dow_labels[d], d) for d in available_dows],\n",
    "    value=available_dows[0],\n",
    "    description='Day:'\n",
    ")\n",
    "\n",
    "w_block = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Time Block:'\n",
    ")\n",
    "\n",
    "w_beta = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.1,\n",
    "    max=0.9,\n",
    "    step=0.05,\n",
    "    description='β (beta):',\n",
    "    readout_format='.2f'\n",
    ")\n",
    "\n",
    "w_min_move = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=0,\n",
    "    max=50,\n",
    "    step=5,\n",
    "    description='Min bikes:'\n",
    ")\n",
    "\n",
    "w_top_k = widgets.IntSlider(\n",
    "    value=15,\n",
    "    min=5,\n",
    "    max=30,\n",
    "    step=5,\n",
    "    description='Max routes:'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# =============================================================================\n",
    "# UPDATE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def update_block_options(*args):\n",
    "    \"\"\"Update block dropdown based on selected month and dow.\"\"\"\n",
    "    month = w_month.value\n",
    "    dow = w_dow.value\n",
    "    \n",
    "    # Get blocks for this month×dow\n",
    "    blocks_for_pair = blocks_catalog[\n",
    "        (blocks_catalog['month'] == month) &\n",
    "        (blocks_catalog['dow'] == dow)\n",
    "    ].sort_values('hour_from')\n",
    "    \n",
    "    if blocks_for_pair.empty:\n",
    "        w_block.options = [('No blocks available', None)]\n",
    "    else:\n",
    "        w_block.options = [(row['block'], f\"{row['hour_from']}-{row['hour_to']}\") \n",
    "                           for _, row in blocks_for_pair.iterrows()]\n",
    "\n",
    "def draw_map(*args):\n",
    "    \"\"\"Draw the rebalancing map.\"\"\"\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if w_block.value is None:\n",
    "            print(\"No blocks available for this selection\")\n",
    "            return\n",
    "        \n",
    "        # Parse block\n",
    "        hour_from, hour_to = map(int, w_block.value.split('-'))\n",
    "        \n",
    "        # Get moves for this selection\n",
    "        selected_moves = moves_blocks[\n",
    "            (moves_blocks['month'] == w_month.value) &\n",
    "            (moves_blocks['dow'] == w_dow.value) &\n",
    "            (moves_blocks['hour_from'] == hour_from) &\n",
    "            (moves_blocks['hour_to'] == hour_to)\n",
    "        ].copy()\n",
    "        \n",
    "        if selected_moves.empty:\n",
    "            print(\"No zones need intervention in this time block\")\n",
    "            return\n",
    "        \n",
    "        # Apply β scaling and min_move filter\n",
    "        selected_moves['qty'] = (selected_moves['movable_adjusted'] * w_beta.value).round()\n",
    "        selected_moves = selected_moves[selected_moves['qty'] >= w_min_move.value]\n",
    "        \n",
    "        if selected_moves.empty:\n",
    "            print(f\"No zones have intervention needs ≥ {w_min_move.value} bikes after β scaling\")\n",
    "            return\n",
    "        \n",
    "        # Get zone centroids\n",
    "        centroids = get_zone_centroids(stations_top)\n",
    "        zone_data = selected_moves.merge(centroids, on='zone', how='left')\n",
    "        \n",
    "        supply = zone_data[zone_data['action'] == 'supply'][['zone', 'lat', 'lon', 'qty']].copy()\n",
    "        demand = zone_data[zone_data['action'] == 'demand'][['zone', 'lat', 'lon', 'qty']].copy()\n",
    "        \n",
    "        # Create map\n",
    "        center_lat = stations_top['lat'].mean()\n",
    "        center_lon = stations_top['lon'].mean()\n",
    "        m = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles='cartodbpositron')\n",
    "        \n",
    "        # Add station markers (background)\n",
    "        for _, row in stations_top.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                [row['lat'], row['lon']],\n",
    "                radius=2,\n",
    "                color='#cccccc',\n",
    "                fill=True,\n",
    "                fill_opacity=0.3,\n",
    "                opacity=0.3\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add zone hulls\n",
    "        zones_shown = list(supply['zone']) + list(demand['zone'])\n",
    "        if zones_shown:\n",
    "            hulls = create_zone_hulls_geojson(stations_top, zones_shown)\n",
    "            \n",
    "            def style_function(feature):\n",
    "                zone = feature['properties']['zone']\n",
    "                action = zone_data[zone_data['zone'] == zone]['action'].iloc[0] if zone in zone_data['zone'].values else 'neutral'\n",
    "                return {\n",
    "                    'fillColor': '#a6dfb3' if action == 'supply' else '#f6b0b0',\n",
    "                    'color': '#2e8b57' if action == 'supply' else '#c0392b',\n",
    "                    'weight': 1.5,\n",
    "                    'fillOpacity': 0.3\n",
    "                }\n",
    "            \n",
    "            folium.GeoJson(hulls, style_function=style_function).add_to(m)\n",
    "        \n",
    "        # Greedy matching\n",
    "        routes = greedy_matching(supply, demand, top_k=w_top_k.value)\n",
    "        \n",
    "        if not routes.empty:\n",
    "            max_bikes = routes['bikes_moved'].max()\n",
    "            \n",
    "            for _, route in routes.iterrows():\n",
    "                s_zone = centroids[centroids['zone'] == route['supply_zone']].iloc[0]\n",
    "                d_zone = centroids[centroids['zone'] == route['demand_zone']].iloc[0]\n",
    "                \n",
    "                # Draw route line\n",
    "                weight = 1 + 6 * (route['bikes_moved'] / max_bikes)\n",
    "                folium.PolyLine(\n",
    "                    [(s_zone['lat'], s_zone['lon']), (d_zone['lat'], d_zone['lon'])],\n",
    "                    color='#555555',\n",
    "                    weight=weight,\n",
    "                    opacity=0.8\n",
    "                ).add_to(m)\n",
    "                \n",
    "                # Add label at midpoint\n",
    "                mid_lat = (s_zone['lat'] + d_zone['lat']) / 2\n",
    "                mid_lon = (s_zone['lon'] + d_zone['lon']) / 2\n",
    "                folium.Marker(\n",
    "                    [mid_lat, mid_lon],\n",
    "                    icon=folium.DivIcon(html=f'''\n",
    "                        <div style=\"font-weight:700; background:white; padding:2px 6px; \n",
    "                                    border-radius:6px; opacity:0.9; font-size:11px;\">\n",
    "                            {format_bikes(route['bikes_moved'])}\n",
    "                        </div>\n",
    "                    ''')\n",
    "                ).add_to(m)\n",
    "        \n",
    "        # Add zone center markers\n",
    "        for _, row in supply.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                [row['lat'], row['lon']],\n",
    "                radius=7,\n",
    "                color='#2e8b57',\n",
    "                fill=True,\n",
    "                fill_opacity=0.9,\n",
    "                popup=f\"Zone {row['zone']}: Supply {int(row['qty'])} bikes\"\n",
    "            ).add_to(m)\n",
    "        \n",
    "        for _, row in demand.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                [row['lat'], row['lon']],\n",
    "                radius=7,\n",
    "                color='#c0392b',\n",
    "                fill=True,\n",
    "                fill_opacity=0.9,\n",
    "                popup=f\"Zone {row['zone']}: Demand {int(row['qty'])} bikes\"\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add title\n",
    "        month_name = pd.Period(w_month.value).strftime('%B')\n",
    "        title_html = f'''\n",
    "        <div style=\"position: fixed; top: 10px; left: 50%; transform: translateX(-50%);\n",
    "                    z-index: 9999; background: rgba(255,255,255,0.95); padding: 6px 12px;\n",
    "                    border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "            <b>CitiBike Rebalancing — {dow_labels[w_dow.value]} {hour_from:02d}:00–{hour_to:02d}:00 ({month_name})</b>\n",
    "        </div>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        # Fit bounds\n",
    "        bounds = [[stations_top['lat'].min()-0.01, stations_top['lon'].min()-0.01],\n",
    "                  [stations_top['lat'].max()+0.01, stations_top['lon'].max()+0.01]]\n",
    "        m.fit_bounds(bounds)\n",
    "        \n",
    "        display(m)\n",
    "\n",
    "# =============================================================================\n",
    "# BIND OBSERVERS\n",
    "# =============================================================================\n",
    "\n",
    "w_month.observe(lambda change: (update_block_options(), draw_map()), names='value')\n",
    "w_dow.observe(lambda change: (update_block_options(), draw_map()), names='value')\n",
    "w_block.observe(draw_map, names='value')\n",
    "w_beta.observe(draw_map, names='value')\n",
    "w_min_move.observe(draw_map, names='value')\n",
    "w_top_k.observe(draw_map, names='value')\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize\n",
    "update_block_options()\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([w_month, w_dow, w_block]),\n",
    "    widgets.HBox([w_beta, w_min_move, w_top_k])\n",
    "])\n",
    "\n",
    "display(controls, output)\n",
    "draw_map()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERACTIVE MAP READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nControls:\")\n",
    "print(\"- Month/Day/Time Block: Select when to view\")\n",
    "print(\"- β (beta): Fraction of movable bikes to actually move (0.1=conservative, 0.9=aggressive)\")\n",
    "print(\"- Min bikes: Hide zones with recommendations below this threshold\")\n",
    "print(\"- Max routes: Limit number of supply→demand routes shown\")\n",
    "print(\"\\nColors:\")\n",
    "print(\"- Green zones = Supply (excess bikes)\")\n",
    "print(\"- Red zones = Demand (need bikes)\")\n",
    "print(\"- Lines = Suggested redistribution routes (thickness = quantity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73fe59-76bc-48e8-aefc-3a30280fe93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CB22)",
   "language": "python",
   "name": "cb22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
